{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59c2618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pirgh\\miniconda3\\envs\\melanoma_5060\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training Engine Locked to Blackwell (sm_120)\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 0: THE BLACKWELL OPTIMIZER (sm_120)\n",
    "import torch, timm, gc, time, os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler # 2026 updated amp location\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# PERFORMANCE FLAGS FOR 50-SERIES\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True # Locks in optimized kernels for 384px size\n",
    "device = torch.device('cuda')\n",
    "\n",
    "print(f\"‚úÖ Training Engine Locked to Blackwell (sm_120)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ready on: NVIDIA GeForce RTX 5060 Laptop GPU\n",
      "‚úÖ Model: tf_efficientnet_b4_ns (heavy), Image: 380px, Batch: 32\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 1: LOCAL CONFIGURATION\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "class Config:\n",
    "    seed = 42\n",
    "    model_name = 'tf_efficientnet_b1_ns'  # B1 = fast but still good accuracy\n",
    "    image_size = 256  # Smaller = faster\n",
    "    \n",
    "    batch_size = 128  # Larger batch = higher throughput\n",
    "    accumulation_steps = 1 \n",
    "    \n",
    "    epochs = 12\n",
    "    learning_rate = 2e-4  # Higher LR for larger batch\n",
    "    fold = 0\n",
    "    data_dir = r\"E:\\Data_Mining_Project\\Data\" \n",
    "    \n",
    "    num_workers = 0\n",
    "    prefetch_factor = None\n",
    "    \n",
    "    meta_features = [\n",
    "        'sex', 'age_approx', 'n_images', 'image_size',\n",
    "        'site_head/neck', 'site_lower extremity', 'site_oral/genital', \n",
    "        'site_palms/soles', 'site_torso', 'site_upper extremity', 'site_nan'\n",
    "    ]\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    cudnn.benchmark = True \n",
    "    cudnn.deterministic = False\n",
    "\n",
    "seed_everything(Config.seed)\n",
    "\n",
    "print(f\"‚úÖ Ready on: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"‚úÖ Model: {Config.model_name}, Image: {Config.image_size}px, Batch: {Config.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3712e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Calculating local image file sizes using Parallel Threads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Sizes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33126/33126 [00:00<00:00, 106951.69it/s]\n",
      "Test Sizes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10982/10982 [00:00<00:00, 40571.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing complete! Memory usage reduced via float32/int8.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_size</th>\n",
       "      <th>n_images</th>\n",
       "      <th>target</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_2637011</td>\n",
       "      <td>14.425748</td>\n",
       "      <td>4.753590</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015719</td>\n",
       "      <td>14.463459</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0052212</td>\n",
       "      <td>11.838734</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0068279</td>\n",
       "      <td>12.281898</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0074268</td>\n",
       "      <td>14.316252</td>\n",
       "      <td>3.044523</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  image_size  n_images  target  fold\n",
       "0  ISIC_2637011   14.425748  4.753590       0     4\n",
       "1  ISIC_0015719   14.463459  3.218876       0     1\n",
       "2  ISIC_0052212   11.838734  1.791759       0     2\n",
       "3  ISIC_0068279   12.281898  3.135494       0     1\n",
       "4  ISIC_0074268   14.316252  3.044523       0     2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CHUNK 2: HIGH-PERFORMANCE FEATURE ENGINEERING\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.contrib.concurrent import thread_map  # Fast parallel I/O\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 1. Load CSVs with specific dtypes to save RAM immediately\n",
    "train_csv_path = os.path.join(Config.data_dir, 'train.csv')\n",
    "test_csv_path = os.path.join(Config.data_dir, 'test.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_csv_path)\n",
    "df_test = pd.read_csv(test_csv_path)\n",
    "\n",
    "# 2. Fix Windows Paths (Raw strings + efficient join)\n",
    "df_train['filepath'] = df_train['image_name'].apply(lambda x: os.path.join(Config.data_dir, 'jpeg', 'train', f'{x}.jpg'))\n",
    "df_test['filepath'] = df_test['image_name'].apply(lambda x: os.path.join(Config.data_dir, 'jpeg', 'test', f'{x}.jpg'))\n",
    "\n",
    "# --- CLEVER FEATURE 1: n_images (Patient Image Count) ---\n",
    "concat = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
    "patient_counts = concat.groupby('patient_id')['image_name'].count().to_dict()\n",
    "df_train['n_images'] = df_train['patient_id'].map(patient_counts).fillna(1)\n",
    "df_test['n_images'] = df_test['patient_id'].map(patient_counts).fillna(1)\n",
    "\n",
    "# Apply log1p for normalization\n",
    "df_train['n_images'] = np.log1p(df_train['n_images'].values).astype(np.float32)\n",
    "df_test['n_images'] = np.log1p(df_test['n_images'].values).astype(np.float32)\n",
    "\n",
    "# --- CLEVER FEATURE 2: image_size (Optimized Parallel Calculation) ---\n",
    "print(\"üîç Calculating local image file sizes using Parallel Threads...\")\n",
    "\n",
    "def safe_get_size(path):\n",
    "    try:\n",
    "        return os.path.getsize(path)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# thread_map is significantly faster than a for-loop for checking 100k+ files on an SSD\n",
    "train_sizes = thread_map(safe_get_size, df_train['filepath'].values, max_workers=8, desc=\"Train Sizes\")\n",
    "test_sizes = thread_map(safe_get_size, df_test['filepath'].values, max_workers=8, desc=\"Test Sizes\")\n",
    "\n",
    "df_train['image_size'] = np.log(np.array(train_sizes, dtype=np.float32) + 1e-6)\n",
    "df_test['image_size'] = np.log(np.array(test_sizes, dtype=np.float32) + 1e-6)\n",
    "\n",
    "# 3. Metadata Mapping\n",
    "# One-hot encoding anatomy sites\n",
    "dummies_train = pd.get_dummies(df_train['anatom_site_general_challenge'], prefix='site', dummy_na=True)\n",
    "df_train = pd.concat([df_train, dummies_train], axis=1)\n",
    "\n",
    "# 4. Standard Preprocessing & Downcasting\n",
    "df_train['sex'] = df_train['sex'].map({'male': 1, 'female': 0}).fillna(-1).astype(np.int8)\n",
    "df_train['age_approx'] = (df_train['age_approx'].fillna(0) / 90.0).astype(np.float32)\n",
    "\n",
    "# Ensure all meta-feature columns exist\n",
    "for col in Config.meta_features:\n",
    "    if col not in df_train.columns:\n",
    "        df_train[col] = 0\n",
    "    df_train[col] = df_train[col].astype(np.float32)\n",
    "\n",
    "# 5. Stratified 5-Fold Split\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.seed)\n",
    "df_train['fold'] = -1\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n",
    "    df_train.loc[val_idx, 'fold'] = fold\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete! Memory usage reduced via float32/int8.\")\n",
    "display(df_train[['image_name', 'image_size', 'n_images', 'target', 'fold']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6fd7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d7316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset class and 1st Place Augmentations ready.\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 3: OPTIMIZED LOCAL DATASET & 1st PLACE AUGMENTATIONS\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_transforms(image_size):\n",
    "    # SPEED-OPTIMIZED: Minimal CPU augmentations (images pre-resized in cache)\n",
    "    # Heavy augmentations cause CPU bottleneck -> low GPU util\n",
    "    train_transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    valid_transform = A.Compose([\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    return train_transform, valid_transform\n",
    "\n",
    "class MelanomaDataset(Dataset):\n",
    "    def __init__(self, df, mode='train', meta_features=None, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.mode = mode\n",
    "        self.use_meta = meta_features is not None\n",
    "        self.meta_features = meta_features\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Faster Image Loading\n",
    "        img_path = self.df.loc[index, 'filepath']\n",
    "        \n",
    "        # Windows performance tip: cv2.imread is generally faster than PIL\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            # Fallback for corrupted/missing files in 106GB dump\n",
    "            image = np.zeros((Config.image_size, Config.image_size, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 2. Apply Augmentations\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        \n",
    "        # 3. Handle Meta-features\n",
    "        if self.use_meta:\n",
    "            meta = self.df.loc[index, self.meta_features].values.astype(np.float32)\n",
    "            data = (image, torch.tensor(meta))\n",
    "        else:\n",
    "            data = image\n",
    "            \n",
    "        if self.mode == 'test':\n",
    "            return data\n",
    "        else:\n",
    "            label = self.df.loc[index, 'target']\n",
    "            return data, torch.tensor(label).float()\n",
    "\n",
    "# Initialize transforms\n",
    "train_transform, valid_transform = get_transforms(Config.image_size)\n",
    "print(\"‚úÖ Dataset class and 1st Place Augmentations ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a93ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pirgh\\miniconda3\\envs\\melanoma_5060\\lib\\site-packages\\timm\\models\\_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_b4_ns to current tf_efficientnet_b4.ns_jft_in1k.\n",
      "  model = create_fn(\n",
      "c:\\Users\\pirgh\\miniconda3\\envs\\melanoma_5060\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pirgh\\.cache\\huggingface\\hub\\models--timm--tf_efficientnet_b4.ns_jft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Ready on CUDA: NVIDIA GeForce RTX 5060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 4: CUDA-OPTIMIZED ARCHITECTURE\n",
    "\n",
    "import timm\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "class Effnet_Melanoma(nn.Module):\n",
    "    def __init__(self, enet_type, n_meta_features=0, n_meta_dim=[512, 128], out_dim=1, pretrained=True):\n",
    "        super(Effnet_Melanoma, self).__init__()\n",
    "        self.n_meta_features = n_meta_features\n",
    "        \n",
    "        # 1. EfficientNet Backbone\n",
    "        self.enet = timm.create_model(enet_type, pretrained=pretrained)\n",
    "        in_ch = self.enet.classifier.in_features\n",
    "        self.enet.classifier = nn.Identity()\n",
    "\n",
    "        # 2. Meta-data Branch\n",
    "        if n_meta_features > 0:\n",
    "            self.meta = nn.Sequential(\n",
    "                nn.Linear(n_meta_features, n_meta_dim[0]),\n",
    "                nn.BatchNorm1d(n_meta_dim[0]),\n",
    "                nn.SiLU(inplace=True),\n",
    "                nn.Dropout(p=0.3),\n",
    "                nn.Linear(n_meta_dim[0], n_meta_dim[1]),\n",
    "                nn.BatchNorm1d(n_meta_dim[1]),\n",
    "                nn.SiLU(inplace=True),\n",
    "            )\n",
    "            in_ch += n_meta_dim[1]\n",
    "\n",
    "        # 3. Multi-Sample Dropout\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n",
    "        self.myfc = nn.Linear(in_ch, out_dim)\n",
    "\n",
    "    def forward(self, x, x_meta=None):\n",
    "        # 1. Image features\n",
    "        x = self.enet(x)\n",
    "        \n",
    "        # 2. Meta features\n",
    "        if self.n_meta_features > 0:\n",
    "            x_meta = self.meta(x_meta)\n",
    "            x = torch.cat((x, x_meta), dim=1)\n",
    "        \n",
    "        # 3. Multi-Sample Inference\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                out = self.myfc(dropout(x))\n",
    "            else:\n",
    "                out += self.myfc(dropout(x))\n",
    "        \n",
    "        return out / len(self.dropouts)\n",
    "\n",
    "# Create model and move to CUDA\n",
    "model = Effnet_Melanoma(\n",
    "    enet_type=Config.model_name,\n",
    "    n_meta_features=len(Config.meta_features)\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ Model Ready on CUDA: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3733c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Preloading images to RAM (eliminates disk I/O bottleneck)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26500/26500 [05:44<00:00, 76.94it/s] \n",
      "Valid: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6626/6626 [01:35<00:00, 69.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cached 26500 train + 6626 valid images in RAM\n",
      "‚úÖ Train batches: 828, Valid batches: 208\n",
      "‚úÖ Training setup ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CHUNK 5: TRAINING LOOP WITH RAM CACHE\n",
    "\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- Setup DataLoaders ---\n",
    "train_df = df_train[df_train['fold'] != Config.fold].reset_index(drop=True)\n",
    "valid_df = df_train[df_train['fold'] == Config.fold].reset_index(drop=True)\n",
    "\n",
    "# --- PRELOAD IMAGES TO RAM ---\n",
    "print(\"üì¶ Preloading images to RAM (eliminates disk I/O bottleneck)...\")\n",
    "\n",
    "def load_and_resize(path):\n",
    "    try:\n",
    "        img = cv2.imread(path)\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (Config.image_size, Config.image_size))\n",
    "            return img\n",
    "    except:\n",
    "        pass\n",
    "    return np.zeros((Config.image_size, Config.image_size, 3), dtype=np.uint8)\n",
    "\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "train_images = thread_map(load_and_resize, train_df['filepath'].values, max_workers=8, desc=\"Train\")\n",
    "valid_images = thread_map(load_and_resize, valid_df['filepath'].values, max_workers=8, desc=\"Valid\")\n",
    "\n",
    "# Cached Dataset\n",
    "class CachedDataset(Dataset):\n",
    "    def __init__(self, df, images, meta_features, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.images = images\n",
    "        self.meta_features = meta_features\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        \n",
    "        meta = self.df.loc[idx, self.meta_features].values.astype(np.float32)\n",
    "        label = self.df.loc[idx, 'target']\n",
    "        return (image, torch.tensor(meta)), torch.tensor(label).float()\n",
    "\n",
    "train_dataset = CachedDataset(train_df, train_images, Config.meta_features, train_transform)\n",
    "valid_dataset = CachedDataset(valid_df, valid_images, Config.meta_features, valid_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, \n",
    "                          num_workers=0, pin_memory=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=Config.batch_size, shuffle=False,\n",
    "                          num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"‚úÖ Cached {len(train_images)} train + {len(valid_images)} valid images in RAM\")\n",
    "print(f\"‚úÖ Train batches: {len(train_loader)}, Valid batches: {len(valid_loader)}\")\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.epochs)\n",
    "scaler = GradScaler()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, scaler, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    \n",
    "    bar = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for (images, meta), labels in bar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        meta = meta.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast('cuda'):\n",
    "            output = model(images, meta)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        bar.set_postfix(loss=np.mean(train_loss[-50:]))\n",
    "        \n",
    "    return np.mean(train_loss)\n",
    "\n",
    "print(\"‚úÖ Training setup ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061aa4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Local Training on RTX 5060 (Fold 0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/828 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# --- 1. Validation Function ---\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images, meta), labels in tqdm(loader, desc=\"Validating\"):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            meta = meta.to(device, non_blocking=True)\n",
    "\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                output = model(images, meta)\n",
    "\n",
    "            # Sigmoid is needed because we use BCEWithLogitsLoss (no sigmoid in model)\n",
    "            preds.append(torch.sigmoid(output).detach().cpu().numpy())\n",
    "            targets.append(labels.numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "    # Binary ROC-AUC\n",
    "    auc = roc_auc_score(targets, preds)\n",
    "    return auc\n",
    "\n",
    "# --- 2. Main Training Loop ---\n",
    "best_auc = 0.0\n",
    "print(f\"üöÄ Starting Local Training on RTX 5060 (Fold {Config.fold})...\")\n",
    "\n",
    "for epoch in range(Config.epochs):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, device, epoch)\n",
    "\n",
    "    # Validate\n",
    "    current_auc = validate(model, valid_loader, device)\n",
    "\n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Logic to save the best model\n",
    "    if current_auc > best_auc:\n",
    "        best_auc = current_auc\n",
    "        torch.save(model.state_dict(), f\"best_model_fold{Config.fold}.pth\")\n",
    "        print(f\"‚≠ê NEW BEST AUC: {best_auc:.4f} | Model Saved!\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1} AUC: {current_auc:.4f} (Best: {best_auc:.4f})\")\n",
    "\n",
    "    # Memory Cleanup\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nüèÜ Local Training Complete!\")\n",
    "print(f\"Highest Score Achieved: {best_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melanoma_5060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
